{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pose_estimator_2d import openpose_estimator\n",
    "from utils import smooth, vis, camera\n",
    "from bvh_skeleton import openpose_skeleton, h36m_skeleton, cmu_skeleton\n",
    "\n",
    "import cv2\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate 2D pose from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    pose_landmarks_list = detection_result.pose_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected poses to visualize.\n",
    "    for idx in range(len(pose_landmarks_list)):\n",
    "        pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "        # Draw the pose landmarks.\n",
    "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        pose_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            pose_landmarks_proto,\n",
    "            solutions.pose.POSE_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "        # Add numbers beside each landmark\n",
    "        for i, landmark in enumerate(pose_landmarks):\n",
    "            x = int(landmark.x * annotated_image.shape[1])\n",
    "            y = int(landmark.y * annotated_image.shape[0])\n",
    "            cv2.putText(annotated_image, str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('trial.mp4')\n",
    "\n",
    "# Define the MediaPipe to OpenPose joint mapping (33 to 25)\n",
    "# Define the MediaPipe to OpenPose joint mapping (33 to 25)\n",
    "# Note: OpenPose has only 25 keypoints, so the mapping should only go from 0 to 24\n",
    "mediapipe_to_openpose = {\n",
    "    0: 0,  # Nose\n",
    "    1: 1,  # Left Eye (MediaPipe -> OpenPose's Left Eye)\n",
    "    2: 2,  # Right Eye (MediaPipe -> OpenPose's Right Eye)\n",
    "    3: 3,  # Left Ear (MediaPipe -> OpenPose's Left Ear)\n",
    "    4: 4,  # Right Ear (MediaPipe -> OpenPose's Right Ear)\n",
    "    5: 5,  # Left Shoulder\n",
    "    6: 6,  # Right Shoulder\n",
    "    7: 7,  # Left Elbow\n",
    "    8: 8,  # Right Elbow\n",
    "    9: 9,  # Left Wrist\n",
    "    10: 10,  # Right Wrist\n",
    "    11: 11,  # Left Hip\n",
    "    12: 12,  # Right Hip\n",
    "    13: 13,  # Left Knee\n",
    "    14: 14,  # Right Knee\n",
    "    15: 15,  # Left Ankle\n",
    "    16: 16,  # Right Ankle\n",
    "    17: 17,  # Left Heel\n",
    "    18: 18,  # Right Heel\n",
    "    19: 19,  # Left Foot Index\n",
    "    20: 20,  # Right Foot Index\n",
    "    21: 21,  # Left Thumb (can be ignored for OpenPose)\n",
    "    22: 22,  # Right Thumb (can be ignored for OpenPose)\n",
    "    23: 23,  # Left Little Finger (can be ignored for OpenPose)\n",
    "    24: 24,  # Right Little Finger (can be ignored for OpenPose)\n",
    "    # Remove any joint mappings for indices >= 25\n",
    "}\n",
    "\n",
    "# Now, process the frame and apply the mapping\n",
    "keypoints_list = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_height = frame.shape[0]\n",
    "    img_width = frame.shape[1]\n",
    "    # Convert the image to RGB (MediaPipe uses RGB)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect pose landmarks using MediaPipe\n",
    "    result = pose.process(rgb_frame)\n",
    "    \n",
    "    if result.pose_landmarks:\n",
    "        keypoints = np.zeros((25, 3))  # OpenPose has 25 keypoints\n",
    "        \n",
    "        # Map MediaPipe landmarks to OpenPose 25 keypoints\n",
    "        for i, landmark in enumerate(result.pose_landmarks.landmark):\n",
    "            if i in mediapipe_to_openpose:\n",
    "                openpose_idx = mediapipe_to_openpose[i]\n",
    "                keypoints[openpose_idx] = [landmark.x, landmark.y, landmark.z]\n",
    "\n",
    "        # Append the 25 keypoints to the keypoints list\n",
    "        keypoints_list.append(keypoints)\n",
    "    else:\n",
    "        keypoints_list.append(None)\n",
    "\n",
    "    # Optional: Visualize the results\n",
    "    annotated_image = frame.copy()\n",
    "    for landmark in result.pose_landmarks.landmark:\n",
    "        x = int(landmark.x * frame.shape[1])\n",
    "        y = int(landmark.y * frame.shape[0])\n",
    "        cv2.circle(annotated_image, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "    # Show the frame with keypoints\n",
    "    cv2.imshow('Pose Detection', annotated_image)\n",
    "    \n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Now you have the keypoints_list that contains 25 keypoints for each frame\n",
    "# You can use it as you need for further processing or saving it in a desired format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 2D pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out failed result\n",
    "keypoints_list = smooth.filter_missing_value(\n",
    "    keypoints_list=keypoints_list,\n",
    "    method='ignore' # interpolation method will be implemented later\n",
    ")\n",
    "\n",
    "# smooth process will be implemented later\n",
    "\n",
    "# save 2d pose result\n",
    "pose2d = np.stack(keypoints_list)[:, :, :2]\n",
    "pose2d_file = Path('2d_pose.npy')\n",
    "np.save(pose2d_file, pose2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 2D pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set the path to save the visualized images\n",
    "vis_result_dir = Path('2d_pose_vis')\n",
    "\n",
    "# Ensure the directory exists\n",
    "vis_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(str('trial.mp4'))\n",
    "op_skel = openpose_skeleton.OpenPoseSkeleton()\n",
    "\n",
    "for i, keypoints in enumerate(keypoints_list):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # keypoint whose detect confidence under kp_thresh will not be visualized\n",
    "    output_file = vis_result_dir / f'{i:04d}.png'  # Correct path joining\n",
    "    vis.vis_2d_keypoints(\n",
    "        keypoints=keypoints,\n",
    "        img=frame,\n",
    "        skeleton=op_skel,\n",
    "        kp_thresh=0.4,\n",
    "        output_file=str(output_file)  # Convert to string if necessary\n",
    "    )\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize 3D pose estimator (Still working on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib\n",
    "from pose_estimator_3d import estimator_3d\n",
    "\n",
    "# Reload the module to apply changes if necessary\n",
    "importlib.reload(estimator_3d)\n",
    "\n",
    "# Use plain strings for file paths\n",
    "config_file = Path('models/linear_model.yaml')\n",
    "config_file = str(config_file.resolve())  # Ensure it is an absolute path\n",
    "# Use plain strings for file paths\n",
    "checkpoint_file = Path('models/best_64.12.pth')\n",
    "checkpoint_file = str(checkpoint_file.resolve()) \n",
    "\n",
    "e3d = estimator_3d.Estimator3D(\n",
    "    config_file=config_file,\n",
    "    checkpoint_file=checkpoint_file\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate 3D pose from 2D pose (still working on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose2d = np.load(pose2d_file)\n",
    "pose3d = e3d.estimate(pose2d, image_width=img_width, image_height=img_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 3D pose from camera coordinates to world coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose3d=np.load(\"../../waving_human_openpose_3d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove camera to world transformation if not needed\n",
    "pose3d_world = pose3d  # Data is already in world coordinates\n",
    "\n",
    "pose3d_file =  '3d_pose.npy'\n",
    "np.save(pose3d_file, pose3d_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 3D pose to BVH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvh_file = f'{\"hellococo\"}.bvh'\n",
    "cmu_skel = cmu_skeleton.CMUSkeleton()\n",
    "channels, header = cmu_skel.poses2bvh(pose3d_world, output_file=bvh_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvh_file = f'{\"helloopen\"}.bvh'\n",
    "open = openpose_skeleton.OpenPoseSkeleton()\n",
    "channels, header = open.poses2bvh(pose3d_world, output_file=bvh_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'miscs/h36m_cxk.bvh'\n",
    "h36m_skel = h36m_skeleton.H36mSkeleton()\n",
    "_ = h36m_skel.poses2bvh(pose3d_world, output_file=output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
