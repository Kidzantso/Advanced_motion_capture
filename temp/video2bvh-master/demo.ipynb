{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pose_estimator_2d import openpose_estimator\n",
    "from utils import smooth, vis, camera\n",
    "from bvh_skeleton import openpose_skeleton, h36m_skeleton, cmu_skeleton\n",
    "\n",
    "import cv2\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate 2D pose from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Function to draw landmarks on the image\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    pose_landmarks_list = detection_result.pose_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected poses to visualize.\n",
    "    for idx in range(len(pose_landmarks_list)):\n",
    "        pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "        # Draw the pose landmarks.\n",
    "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        pose_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            pose_landmarks_proto,\n",
    "            solutions.pose.POSE_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "        # Add numbers beside each landmark\n",
    "        for i, landmark in enumerate(pose_landmarks):\n",
    "            x = int(landmark.x * annotated_image.shape[1])\n",
    "            y = int(landmark.y * annotated_image.shape[0])\n",
    "            cv2.putText(annotated_image, str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose detection completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the new MediaPipe Pose detector\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('miscs/cxk.mp4')\n",
    "\n",
    "# Define the MediaPipe to OpenPose joint mapping (33 to 25)\n",
    "mediapipe_to_openpose = {\n",
    "    0: 0,  # Nose\n",
    "    11: 11,  # Left Hip\n",
    "    12: 12,  # Right Hip\n",
    "    23: 13,  # Left Knee\n",
    "    24: 14,  # Right Knee\n",
    "    25: 15,  # Left Ankle\n",
    "    26: 16,  # Right Ankle\n",
    "    27: 17,  # Left Heel\n",
    "    28: 18,  # Right Heel\n",
    "    31: 19,  # Left Foot Index\n",
    "    32: 20,  # Right Foot Index\n",
    "    13: 5,  # Left Shoulder\n",
    "    14: 6,  # Right Shoulder\n",
    "    15: 7,  # Left Elbow\n",
    "    16: 8,  # Right Elbow\n",
    "    17: 9,  # Left Wrist\n",
    "    18: 10,  # Right Wrist\n",
    "}\n",
    "\n",
    "# Process video frames and extract pose landmarks\n",
    "keypoints_list = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_height, img_width = frame.shape[:2]\n",
    "    # Convert the image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect pose landmarks\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "    detection_result = detector.detect(mp_image)\n",
    "\n",
    "    if detection_result.pose_landmarks:\n",
    "        keypoints = np.zeros((25, 3))  # OpenPose has 25 keypoints\n",
    "\n",
    "        # Map MediaPipe landmarks to OpenPose 25 keypoints\n",
    "        for mp_idx, openpose_idx in mediapipe_to_openpose.items():\n",
    "            if mp_idx < len(detection_result.pose_landmarks[0]):\n",
    "                landmark = detection_result.pose_landmarks[0][mp_idx]\n",
    "                keypoints[openpose_idx] = [landmark.x, landmark.y, landmark.z]\n",
    "\n",
    "        keypoints_list.append(keypoints)\n",
    "    else:\n",
    "        keypoints_list.append(None)\n",
    "\n",
    "    annotated_image = draw_landmarks_on_image(rgb_frame, detection_result)\n",
    "\n",
    "    # Resize the visualization window\n",
    "    cv2.namedWindow('Pose Detection', cv2.WINDOW_NORMAL)  # Make the window resizable\n",
    "    cv2.resizeWindow('Pose Detection', 1500, 750)  # Set the window size \n",
    "\n",
    "    cv2.imshow('Pose Detection', cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save keypoints_list or process further\n",
    "print(\"Pose detection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keypoints_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 2D pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 2d pose result\n",
    "pose2d = np.stack(keypoints_list)[:, :, :2]\n",
    "pose2d_file = Path('2d_pose.npy')\n",
    "np.save(pose2d_file, pose2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (900, 25, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "data = np.load('miscs/cxk_cache/2d_pose.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Shape of the data:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (900, 25, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "data = np.load('2d_pose.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Shape of the data:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 2D pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set the path to save the visualized images\n",
    "vis_result_dir = Path('2d_pose_vis')\n",
    "\n",
    "# Ensure the directory exists\n",
    "vis_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(str('trial.mp4'))\n",
    "op_skel = openpose_skeleton.OpenPoseSkeleton()\n",
    "\n",
    "for i, keypoints in enumerate(keypoints_list):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # keypoint whose detect confidence under kp_thresh will not be visualized\n",
    "    output_file = vis_result_dir / f'{i:04d}.png'  # Correct path joining\n",
    "    vis.vis_2d_keypoints(\n",
    "        keypoints=keypoints,\n",
    "        img=frame,\n",
    "        skeleton=op_skel,\n",
    "        kp_thresh=0.4,\n",
    "        output_file=str(output_file)  # Convert to string if necessary\n",
    "    )\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize 3D pose estimator (Still working on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Read 3D estimator config from C:\\Users\\dodom\\OneDrive - Misr International University\\Desktop\\College\\college\\Graduation Project\\video2bvh-master\\video2bvh-master\\models\\video_pose.yaml.\n",
      "{'DATASET': {'CAM_PARAMS': '/home/kevin/public98/3dpose/Dataset/h36m/cameras.h5',\n",
      "             'IMAGE_HEIGHT': 1002,\n",
      "             'IMAGE_WIDTH': 1000,\n",
      "             'INPUT_LEFT_JOINTS': [5, 6, 7, 12, 13, 14, 16, 18, 19, 20, 21],\n",
      "             'INPUT_RIGHT_JOINTS': [2, 3, 4, 9, 10, 11, 15, 17, 22, 23, 24],\n",
      "             'INPUT_ROOT': '/home/kevin/HDD/h36m_dataset/2D_openpose',\n",
      "             'IN_CHANNEL': 2,\n",
      "             'IN_JOINT': 25,\n",
      "             'NAME': 'h36m',\n",
      "             'OUTPUT_LEFT_JOINTS': [4, 5, 6, 11, 12, 13],\n",
      "             'OUTPUT_RIGHT_JOINTS': [1, 2, 3, 14, 15, 16],\n",
      "             'OUT_CHANNEL': 3,\n",
      "             'OUT_JOINT': 17,\n",
      "             'SEQ_LEN': 243,\n",
      "             'TARGET_ROOT': '/home/kevin/HDD/h36m_dataset/3D_gt',\n",
      "             'TEST_FLIP': True,\n",
      "             'TRAIN_FLIP': True},\n",
      " 'MODEL': {'ACTIVATION': 'relu',\n",
      "           'BIAS': True,\n",
      "           'DROPOUT': 0.25,\n",
      "           'DSC': False,\n",
      "           'FILTER_WIDTHS': [3, 3, 3, 3, 3],\n",
      "           'HIDDEN_SIZE': 1024,\n",
      "           'NAME': 'video_pose',\n",
      "           'PRETRAIN': '',\n",
      "           'RESIDUAL': True},\n",
      " 'TRAIN': {'AMSGRAD': True,\n",
      "           'BATCH_SIZE': 1024,\n",
      "           'BUFFER_SIZE': 4000000,\n",
      "           'EPOCH': 80,\n",
      "           'EVAL_FREQ': 5,\n",
      "           'LR': 0.001,\n",
      "           'LR_DECAY': 0.95,\n",
      "           'MPJPE_WEIGHT': 1,\n",
      "           'OPTIMIZER': 'adam',\n",
      "           'PRINT_FREQ': 50,\n",
      "           'SNAP_FREQ': 10000,\n",
      "           'WORKERS': 4}}\n",
      "=> Load checkpoint C:\\Users\\dodom\\OneDrive - Misr International University\\Desktop\\College\\college\\Graduation Project\\video2bvh-master\\video2bvh-master\\models\\best_58.58.pth\n",
      "=> Use device cpu.\n",
      "e3d initialized: <pose_estimator_3d.estimator_3d.Estimator3D object at 0x00000252B3293F80>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pathlib\n",
    "import importlib\n",
    "from pose_estimator_3d import estimator_3d \n",
    "\n",
    "# Fix pathlib.PosixPath issue on Windows\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# Reload the `estimator_3d` module to ensure it's fresh\n",
    "importlib.reload(estimator_3d)\n",
    "\n",
    "# Initialize Estimator3D with the configuration and checkpoint\n",
    "e3d = estimator_3d.Estimator3D(\n",
    "    config_file='models/video_pose.yaml',\n",
    "    checkpoint_file='models/best_58.58.pth'\n",
    ")\n",
    "\n",
    "# Revert pathlib changes (optional)\n",
    "pathlib.PosixPath = temp\n",
    "\n",
    "# (Optional) Test the e3d object to ensure it works\n",
    "print(\"e3d initialized:\", e3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate 3D pose from 2D pose (still working on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Begin to estimate 3D poses.\n",
      "900 / 900\n"
     ]
    }
   ],
   "source": [
    "pose2d = np.load('2d_pose.npy')\n",
    "pose3d = e3d.estimate(pose2d, image_width=img_width, image_height=img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (900, 17, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "data3d = np.load('miscs/cxk_cache/3d_pose.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Shape of the data:\", data3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (900, 17, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "pose3d = np.load('3d_pose.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Shape of the data:\", pose3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose3d_world = pose3d  # Data is already in world coordinates\n",
    "\n",
    "pose3d_file =  '3d_pose.npy'\n",
    "np.save(pose3d_file, pose3d_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 3D pose to BVH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted succesfully\n"
     ]
    }
   ],
   "source": [
    "bvh_file = f'{\"hellococo\"}.bvh'\n",
    "cmu_skel = cmu_skeleton.CMUSkeleton()\n",
    "channels, header = cmu_skel.poses2bvh(pose3d_world, output_file=bvh_file)\n",
    "print(\"Converted succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def open_blender_with_bvh(\n",
    "    bvh_file,\n",
    "    blender_executable=r\"C:/Program Files/Blender Foundation/Blender 3.5/blender.exe\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Opens Blender, imports a BVH file, and ensures armature visibility.\n",
    "    \n",
    "    Parameters:\n",
    "        bvh_file (str): Path to the BVH file to be imported.\n",
    "        blender_executable (str): Path to the Blender executable.\n",
    "    \"\"\"\n",
    "    # Check if the BVH file exists\n",
    "    if not os.path.exists(bvh_file):\n",
    "        raise FileNotFoundError(f\"The BVH file '{bvh_file}' does not exist.\")\n",
    "    \n",
    "    # Create a temporary Python script to run in Blender\n",
    "    blender_script = f\"\"\"\n",
    "import bpy\n",
    "import os\n",
    "\n",
    "# Path to the BVH file\n",
    "bvh_file = r\"{os.path.abspath(bvh_file)}\"\n",
    "\n",
    "def import_bvh(filepath, scale=1.0):\n",
    "    # Ensure everything is deselected\n",
    "    bpy.ops.object.select_all(action='DESELECT')\n",
    "\n",
    "    # Import the BVH file\n",
    "    bpy.ops.import_anim.bvh(\n",
    "        filepath=filepath,\n",
    "        axis_forward='-Z',   # Adjust if necessary\n",
    "        axis_up='Y',         # Adjust if necessary\n",
    "        filter_glob=\"*.bvh\",\n",
    "        target='ARMATURE',\n",
    "        global_scale=scale,\n",
    "        frame_start=1,\n",
    "        use_fps_scale=True\n",
    "    )\n",
    "\n",
    "def set_armature_display():\n",
    "    for obj in bpy.context.scene.objects:\n",
    "        if obj.type == 'ARMATURE':\n",
    "            bpy.context.view_layer.objects.active = obj  # Make it the active object\n",
    "            obj.select_set(True)\n",
    "            # Ensure it's visible in the viewport\n",
    "            obj.hide_viewport = False\n",
    "            obj.hide_set(False)\n",
    "            # Change display mode for better visibility\n",
    "            obj.data.display_type = 'STICK'  # Other options: 'ENVELOPE', 'OCTAHEDRAL', etc.\n",
    "\n",
    "# Clear existing scene\n",
    "bpy.ops.wm.read_factory_settings(use_empty=True)\n",
    "\n",
    "# Import BVH file\n",
    "import_bvh(bvh_file, scale=10.0)\n",
    "\n",
    "# Adjust armature visibility and display settings\n",
    "set_armature_display()\n",
    "\n",
    "# Optionally save the Blender file\n",
    "output_blend = os.path.splitext(bvh_file)[0] + \".blend\"\n",
    "bpy.ops.wm.save_as_mainfile(filepath=output_blend)\n",
    "\n",
    "print(\"BVH file imported and processed successfully.\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write the Blender script to a temporary file\n",
    "    script_file = \"import_bvh_to_blender.py\"\n",
    "    with open(script_file, \"w\") as file:\n",
    "        file.write(blender_script)\n",
    "    \n",
    "    # Run Blender with the temporary script\n",
    "    try:\n",
    "        subprocess.run([blender_executable, \"--python\", script_file], check=True)\n",
    "    finally:\n",
    "        # Clean up the temporary script\n",
    "        if os.path.exists(script_file):\n",
    "            os.remove(script_file)\n",
    "\n",
    "# Example usage\n",
    "bvh_file = \"miscs/cxk_cache/cxk.bvh\"  # Replace with the correct path to your BVH file\n",
    "try:\n",
    "    open_blender_with_bvh(bvh_file)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to process BVH file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'miscs/h36m_cxk.bvh'\n",
    "h36m_skel = h36m_skeleton.H36mSkeleton()\n",
    "_ = h36m_skel.poses2bvh(pose3d_world, output_file=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
