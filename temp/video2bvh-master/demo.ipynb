{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "DEBUG:matplotlib:matplotlib data path: c:\\Python\\Lib\\site-packages\\matplotlib\\mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=C:\\Users\\dodom\\.matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is win32\n",
      "DEBUG:matplotlib:CACHEDIR=C:\\Users\\dodom\\.matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from C:\\Users\\dodom\\.matplotlib\\fontlist-v390.json\n"
     ]
    }
   ],
   "source": [
    "#from pose_estimator_2d import openpose_estimator\n",
    "from utils import smooth, vis, camera\n",
    "from bvh_skeleton import openpose_skeleton, h36m_skeleton, cmu_skeleton\n",
    "\n",
    "import cv2\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate 2D pose from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "DEBUG:jax._src.path:etils.epath was not found. Using pathlib for file I/O.\n"
     ]
    }
   ],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Function to draw landmarks on the image\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    pose_landmarks_list = detection_result.pose_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected poses to visualize.\n",
    "    for idx in range(len(pose_landmarks_list)):\n",
    "        pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "        # Draw the pose landmarks.\n",
    "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        pose_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            pose_landmarks_proto,\n",
    "            solutions.pose.POSE_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "        # Add numbers beside each landmark\n",
    "        for i, landmark in enumerate(pose_landmarks):\n",
    "            x = int(landmark.x * annotated_image.shape[1])\n",
    "            y = int(landmark.y * annotated_image.shape[0])\n",
    "            cv2.putText(annotated_image, str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D pose estimation completed\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MediaPipe Pose detector\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('miscs/cxk.mp4')\n",
    "\n",
    "# Define the complete MediaPipe to OpenPose joint mapping (33 to 25)\n",
    "mediapipe_to_openpose = {\n",
    "    0: 0,   # Nose\n",
    "    13: 5,  # Left Shoulder\n",
    "    14: 2,  # Right Shoulder\n",
    "    15: 6,  # Left Elbow\n",
    "    16: 3,  # Right Elbow\n",
    "    17: 7,  # Left Wrist\n",
    "    18: 4,  # Right Wrist\n",
    "    23: 12, # Left Hip\n",
    "    24: 9,  # Right Hip\n",
    "    25: 13, # Left Knee\n",
    "    26: 10, # Right Knee\n",
    "    27: 14, # Left Ankle\n",
    "    28: 11, # Right Ankle\n",
    "    31: 19, # Left Big Toe\n",
    "    32: 22, # Right Big Toe\n",
    "    29: 20, # Left Small Toe\n",
    "    30: 21, # Left Heel\n",
    "    28: 23, # Right Small Toe\n",
    "    27: 24, # Right Heel\n",
    "    1: 16,  # Left Eye\n",
    "    2: 15,  # Right Eye\n",
    "    3: 18,  # Left Ear\n",
    "    4: 17,  # Right Ear\n",
    "}\n",
    "\n",
    "# Add interpolated joints\n",
    "def interpolate_joint(lm1, lm2):\n",
    "    \"\"\"Interpolate between two landmarks.\"\"\"\n",
    "    return [(lm1.x + lm2.x) / 2, (lm1.y + lm2.y) / 2, (lm1.z + lm2.z) / 2]\n",
    "\n",
    "# Process video frames\n",
    "keypoints_list = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_height, img_width = frame.shape[:2]\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "    detection_result = detector.detect(mp_image)\n",
    "\n",
    "    keypoints = np.zeros((25, 3))  # OpenPose format: 25 joints\n",
    "\n",
    "    if detection_result.pose_landmarks:\n",
    "        landmarks = detection_result.pose_landmarks[0]\n",
    "\n",
    "        # Map MediaPipe landmarks to OpenPose\n",
    "        for mp_idx, openpose_idx in mediapipe_to_openpose.items():\n",
    "            if mp_idx < len(landmarks):\n",
    "                landmark = landmarks[mp_idx]\n",
    "                keypoints[openpose_idx, 0] = landmark.x * img_width\n",
    "                keypoints[openpose_idx, 1] = landmark.y * img_height\n",
    "                keypoints[openpose_idx, 2] = 1.0  # Confidence\n",
    "\n",
    "        # Add interpolated joints\n",
    "        # Neck (midpoint between left and right shoulders)\n",
    "        neck = interpolate_joint(landmarks[13], landmarks[14])\n",
    "        keypoints[1] = [neck[0] * img_width, neck[1] * img_height, 1.0]\n",
    "\n",
    "        # MidHip (midpoint between left and right hips)\n",
    "        mid_hip = interpolate_joint(landmarks[23], landmarks[24])\n",
    "        keypoints[8] = [mid_hip[0] * img_width, mid_hip[1] * img_height, 1.0]\n",
    "\n",
    "    keypoints_list.append(keypoints)\n",
    "    annotated_image = draw_landmarks_on_image(rgb_frame, detection_result)\n",
    "    cv2.imshow('Pose Detection', cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Resize the visualization window\n",
    "    cv2.namedWindow('Pose Detection', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Pose Detection', 1500, 750)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"2D pose estimation completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keypoints_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 2D pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 2d pose result\n",
    "pose2d = np.stack(keypoints_list)[:, :, :2]\n",
    "pose2d_file = Path('2d_pose.npy')\n",
    "np.save(pose2d_file, pose2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 777.48505  415.6107 ]\n",
      "  [ 818.7475   459.73688]\n",
      "  [ 745.1361   480.1361 ]\n",
      "  ...\n",
      "  [ 836.4469  1057.2075 ]\n",
      "  [   0.         0.     ]\n",
      "  [ 812.8102  1010.0808 ]]\n",
      "\n",
      " [[ 774.5472   409.54886]\n",
      "  [ 818.75323  453.66135]\n",
      "  [ 745.16364  459.80325]\n",
      "  ...\n",
      "  [ 833.38257 1054.2017 ]\n",
      "  [ 806.92896 1048.4553 ]\n",
      "  [ 812.9307  1004.1141 ]]\n",
      "\n",
      " [[ 774.63214  391.97034]\n",
      "  [ 815.8464   436.0819 ]\n",
      "  [ 742.2773   453.6574 ]\n",
      "  ...\n",
      "  [ 824.64166 1051.2272 ]\n",
      "  [ 795.23975 1048.3055 ]\n",
      "  [ 818.77026 1007.0596 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 924.6487   403.747  ]\n",
      "  [ 912.872    462.6452 ]\n",
      "  [ 836.3551   483.2289 ]\n",
      "  ...\n",
      "  [ 974.73535 1074.8062 ]\n",
      "  [ 954.14636 1074.8384 ]\n",
      "  [ 957.06793 1074.8254 ]]\n",
      "\n",
      " [[ 933.45807  400.83636]\n",
      "  [ 918.81116  459.80594]\n",
      "  [ 836.45636  483.1317 ]\n",
      "  ...\n",
      "  [ 971.799   1074.8057 ]\n",
      "  [ 954.0584  1074.8337 ]\n",
      "  [ 957.14044 1060.0922 ]]\n",
      "\n",
      " [[ 954.06696  397.87613]\n",
      "  [ 924.65173  462.59933]\n",
      "  [ 839.3046   483.21722]\n",
      "  ...\n",
      "  [ 968.8072  1074.8271 ]\n",
      "  [ 951.15234 1074.8523 ]\n",
      "  [ 965.8592  1074.8138 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "data = np.load('miscs/cxk_cache/2d_pose.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 765.38497925  429.10190105]\n",
      "  [ 844.97091293  562.24341989]\n",
      "  [ 699.09885406  547.54277945]\n",
      "  ...\n",
      "  [ 824.2971611  1076.93120956]\n",
      "  [ 816.46642685 1002.90159702]\n",
      "  [ 942.26200104  944.5522213 ]]\n",
      "\n",
      " [[ 771.18896484  415.78434706]\n",
      "  [ 840.21746635  547.24051595]\n",
      "  [ 687.75690079  531.54061317]\n",
      "  ...\n",
      "  [ 822.35578537 1072.17481613]\n",
      "  [ 816.59591675  998.9479351 ]\n",
      "  [ 942.11483002  938.76907825]]\n",
      "\n",
      " [[ 764.46292877  400.8792901 ]\n",
      "  [ 838.46334457  534.97050405]\n",
      "  [ 678.3385849   519.82298613]\n",
      "  ...\n",
      "  [ 820.82662582 1071.71892643]\n",
      "  [ 816.86336517  998.4338522 ]\n",
      "  [ 941.28284454  924.51676369]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 919.19179916  409.35454488]\n",
      "  [ 939.5735836   637.85046101]\n",
      "  [ 849.40126419  644.06329393]\n",
      "  ...\n",
      "  [ 982.46898651 1086.068573  ]\n",
      "  [ 960.3560257  1033.03891897]\n",
      "  [ 913.7951088   951.99985743]]\n",
      "\n",
      " [[ 934.48408127  407.36937761]\n",
      "  [ 944.1975975   642.28283286]\n",
      "  [ 850.44467926  647.26907015]\n",
      "  ...\n",
      "  [ 981.9644165  1095.34189224]\n",
      "  [ 963.8344574  1036.72678471]\n",
      "  [ 950.94509125  990.93851566]]\n",
      "\n",
      " [[ 948.47803116  408.5801053 ]\n",
      "  [ 949.32160378  638.83671999]\n",
      "  [ 845.1137352   646.01089954]\n",
      "  ...\n",
      "  [ 970.1625824  1101.09864235]\n",
      "  [ 964.09881592 1041.46766424]\n",
      "  [ 978.30310822  971.76951885]]]\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "data = np.load('2d_pose.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 2D pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set the path to save the visualized images\n",
    "vis_result_dir = Path('2d_pose_vis')\n",
    "\n",
    "# Ensure the directory exists\n",
    "vis_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(str('trial.mp4'))\n",
    "op_skel = openpose_skeleton.OpenPoseSkeleton()\n",
    "\n",
    "for i, keypoints in enumerate(keypoints_list):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # keypoint whose detect confidence under kp_thresh will not be visualized\n",
    "    output_file = vis_result_dir / f'{i:04d}.png'  # Correct path joining\n",
    "    vis.vis_2d_keypoints(\n",
    "        keypoints=keypoints,\n",
    "        img=frame,\n",
    "        skeleton=op_skel,\n",
    "        kp_thresh=0.4,\n",
    "        output_file=str(output_file)  # Convert to string if necessary\n",
    "    )\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize 3D pose estimator (Still working on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Read 3D estimator config from C:\\Users\\dodom\\OneDrive - Misr International University\\Desktop\\College\\college\\Graduation Project\\video2bvh-master\\video2bvh-master\\models\\linear_model.yaml.\n",
      "{'DATASET': {'CAM_PARAMS': '/home/kevin/public98/3dpose/Dataset/h36m/cameras.h5',\n",
      "             'IMAGE_HEIGHT': 1002,\n",
      "             'IMAGE_WIDTH': 1000,\n",
      "             'INPUT_LEFT_JOINTS': [5, 6, 7, 12, 13, 14, 16, 18, 19, 20, 21],\n",
      "             'INPUT_RIGHT_JOINTS': [2, 3, 4, 9, 10, 11, 15, 17, 22, 23, 24],\n",
      "             'INPUT_ROOT': '/home/kevin/HDD/h36m_dataset/2D_openpose',\n",
      "             'IN_CHANNEL': 2,\n",
      "             'IN_JOINT': 25,\n",
      "             'NAME': 'h36m',\n",
      "             'OUTPUT_LEFT_JOINTS': [4, 5, 6, 11, 12, 13],\n",
      "             'OUTPUT_RIGHT_JOINTS': [1, 2, 3, 14, 15, 16],\n",
      "             'OUT_CHANNEL': 3,\n",
      "             'OUT_JOINT': 17,\n",
      "             'SEQ_LEN': 1,\n",
      "             'TARGET_ROOT': '/home/kevin/HDD/h36m_dataset/3D_gt',\n",
      "             'TEST_FLIP': True,\n",
      "             'TRAIN_FLIP': True},\n",
      " 'MODEL': {'ACTIVATION': 'relu',\n",
      "           'BIAS': True,\n",
      "           'BLOCK_NUM': 2,\n",
      "           'DROPOUT': 0.25,\n",
      "           'HIDDEN_SIZE': 1024,\n",
      "           'NAME': 'linear_model',\n",
      "           'PRETRAIN': '',\n",
      "           'RESIDUAL': True},\n",
      " 'TRAIN': {'AMSGRAD': True,\n",
      "           'BATCH_SIZE': 1024,\n",
      "           'BUFFER_SIZE': 4000000,\n",
      "           'EPOCH': 80,\n",
      "           'EVAL_FREQ': 1,\n",
      "           'LR': 0.001,\n",
      "           'LR_DECAY': 0.95,\n",
      "           'MPJPE_WEIGHT': 1,\n",
      "           'OPTIMIZER': 'adam',\n",
      "           'PRINT_FREQ': 50,\n",
      "           'SNAP_FREQ': 10000,\n",
      "           'WORKERS': 4}}\n",
      "=> Load checkpoint C:\\Users\\dodom\\OneDrive - Misr International University\\Desktop\\College\\college\\Graduation Project\\video2bvh-master\\video2bvh-master\\models\\best_64.12.pth\n",
      "=> Use device cpu.\n",
      "e3d initialized: <pose_estimator_3d.estimator_3d.Estimator3D object at 0x000002D0E489D250>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dodom\\OneDrive - Misr International University\\Desktop\\College\\college\\Graduation Project\\video2bvh-master\\video2bvh-master\\pose_estimator_3d\\model\\factory.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(checkpoint_file,map_location=torch.device('cpu'))['model_state']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pathlib\n",
    "import importlib\n",
    "from pose_estimator_3d import estimator_3d \n",
    "\n",
    "# Fix pathlib.PosixPath issue on Windows\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# Reload the `estimator_3d` module to ensure it's fresh\n",
    "importlib.reload(estimator_3d)\n",
    "\n",
    "# Initialize Estimator3D with the configuration and checkpoint\n",
    "e3d = estimator_3d.Estimator3D(\n",
    "    config_file='models/video_pose.yaml',\n",
    "    checkpoint_file='models/best_58.58.pth'\n",
    ")\n",
    "\n",
    "# Revert pathlib changes (optional)\n",
    "pathlib.PosixPath = temp\n",
    "\n",
    "# (Optional) Test the e3d object to ensure it works\n",
    "print(\"e3d initialized:\", e3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate 3D pose from 2D pose (still working on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Begin to estimate 3D poses.\n",
      "161 / 161\n"
     ]
    }
   ],
   "source": [
    "pose2d = np.load('2d_pose.npy')\n",
    "pose3d = e3d.estimate(pose2d, image_width=img_width, image_height=img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[   0.            0.          922.10063329]\n",
      "  [-128.08416251  -19.41330654  922.77364049]\n",
      "  [ -37.19650811 -271.5757016   637.30918071]\n",
      "  ...\n",
      "  [-180.42006445 -255.89631158 1230.35682424]\n",
      "  [-347.33498528 -146.71871621 1050.82630431]\n",
      "  [-446.6621912  -251.62536351  987.74473772]]\n",
      "\n",
      " [[   0.            0.          922.10063329]\n",
      "  [-128.93817239  -19.08141257  922.4809365 ]\n",
      "  [ -40.23146449 -271.72872311  636.97512645]\n",
      "  ...\n",
      "  [-184.53306805 -256.12075935 1231.82107282]\n",
      "  [-353.64487189 -147.78371802 1051.16152275]\n",
      "  [-456.14752477 -257.54998149  991.01831833]]\n",
      "\n",
      " [[   0.            0.          922.10063329]\n",
      "  [-127.93546685  -23.10065644  921.94295637]\n",
      "  [ -35.9186382  -279.8481499   624.86144092]\n",
      "  ...\n",
      "  [-190.30998272 -260.88255489 1229.46203965]\n",
      "  [-363.84663267 -151.20142688 1055.41885724]\n",
      "  [-474.9994782  -258.39727163  995.44238242]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[   0.            0.          922.10063329]\n",
      "  [-107.03742218  -56.11416764  950.93654898]\n",
      "  [  72.18954569 -279.61302081  678.32848957]\n",
      "  ...\n",
      "  [-127.2179587  -281.65999021 1234.3367965 ]\n",
      "  [-132.91294205 -324.41267408  972.83273663]\n",
      "  [ -57.96573339 -417.9768853   751.12951658]]\n",
      "\n",
      " [[   0.            0.          922.10063329]\n",
      "  [-105.42115024  -56.88825925  951.25474317]\n",
      "  [  66.23907076 -268.54931053  666.56253123]\n",
      "  ...\n",
      "  [-124.57200183 -271.3616571  1241.06666584]\n",
      "  [-136.4283042  -314.36430673  976.33192645]\n",
      "  [ -62.46565733 -402.65736909  759.07941541]]\n",
      "\n",
      " [[   0.            0.          922.10063329]\n",
      "  [-105.02179851  -56.56411198  950.53384425]\n",
      "  [  64.40688473 -266.10777234  664.10705765]\n",
      "  ...\n",
      "  [-122.11251257 -268.35587117 1243.99552665]\n",
      "  [-139.727274   -309.59377002  977.36557354]\n",
      "  [ -70.73236991 -392.45843888  758.04576638]]]\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "data3d = np.load('miscs/cxk_cache/3d_pose.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(data3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[   0.            0.            0.        ]\n",
      "  [-181.11601257   20.93011284  -98.03192902]\n",
      "  [-160.63142395  392.71133423 -455.55709839]\n",
      "  ...\n",
      "  [-198.18856812 -334.14736938  -50.52825165]\n",
      "  [-402.65106201 -122.14024353   -7.50772285]\n",
      "  [-477.01257324  -92.36289978    9.52581024]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [-182.08384705   21.12501907  -98.29653168]\n",
      "  [-162.62481689  394.34857178 -456.06637573]\n",
      "  ...\n",
      "  [-199.91113281 -335.76672363  -50.8671608 ]\n",
      "  [-405.87310791 -123.07092285   -9.77371979]\n",
      "  [-477.39004517  -92.79072571    5.78031588]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [-183.63911438   21.32544327  -98.27884674]\n",
      "  [-164.32064819  396.27877808 -456.73031616]\n",
      "  ...\n",
      "  [-202.03178406 -338.15371704  -50.7162056 ]\n",
      "  [-410.38470459 -126.33040619  -12.05461121]\n",
      "  [-477.7696228   -94.51605225    1.70915389]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [-144.59503174   26.4815979  -115.93972015]\n",
      "  [ -86.97019196  417.74621582 -431.42269897]\n",
      "  ...\n",
      "  [ -90.82582855 -330.30838013  -91.90657043]\n",
      "  [-178.5617218   -38.29088593  -79.66620636]\n",
      "  [-232.29698181   24.22994423   -7.93277836]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [-145.17346191   26.72203827 -116.41296387]\n",
      "  [ -90.84008789  422.7437439  -428.97891235]\n",
      "  ...\n",
      "  [ -91.4839859  -330.5814209   -95.2140274 ]\n",
      "  [-179.32647705  -36.42220306  -81.63696289]\n",
      "  [-234.09906006   24.56403542  -12.22595596]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [-146.09536743   26.23705101 -115.72463989]\n",
      "  [ -94.67256165  422.93096924 -429.08074951]\n",
      "  ...\n",
      "  [ -92.41447449 -330.29180908  -97.27484894]\n",
      "  [-180.19958496  -33.83346939  -81.4119873 ]\n",
      "  [-237.23893738   27.73652649  -14.20585823]]]\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "pose3d = np.load('3d_pose.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(pose3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose3d_world = pose3d  # Data is already in world coordinates\n",
    "\n",
    "pose3d_file =  '3d_pose.npy'\n",
    "np.save(pose3d_file, pose3d_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 3D pose to BVH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted succesfully\n"
     ]
    }
   ],
   "source": [
    "bvh_file = f'{\"hellococo\"}.bvh'\n",
    "cmu_skel = cmu_skeleton.CMUSkeleton()\n",
    "channels, header = cmu_skel.poses2bvh(pose3d_world, output_file=bvh_file)\n",
    "print(\"Converted succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def open_blender_with_bvh(\n",
    "    bvh_file,\n",
    "    blender_executable=r\"C:/Program Files/Blender Foundation/Blender 3.5/blender.exe\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Opens Blender, imports a BVH file, and ensures armature visibility.\n",
    "    \n",
    "    Parameters:\n",
    "        bvh_file (str): Path to the BVH file to be imported.\n",
    "        blender_executable (str): Path to the Blender executable.\n",
    "    \"\"\"\n",
    "    # Check if the BVH file exists\n",
    "    if not os.path.exists(bvh_file):\n",
    "        raise FileNotFoundError(f\"The BVH file '{bvh_file}' does not exist.\")\n",
    "    \n",
    "    # Create a temporary Python script to run in Blender\n",
    "    blender_script = f\"\"\"\n",
    "import bpy\n",
    "import os\n",
    "\n",
    "# Path to the BVH file\n",
    "bvh_file = r\"{os.path.abspath(bvh_file)}\"\n",
    "\n",
    "def import_bvh(filepath, scale=1.0):\n",
    "    # Ensure everything is deselected\n",
    "    bpy.ops.object.select_all(action='DESELECT')\n",
    "\n",
    "    # Import the BVH file\n",
    "    bpy.ops.import_anim.bvh(\n",
    "        filepath=filepath,\n",
    "        axis_forward='-Z',   # Adjust if necessary\n",
    "        axis_up='Y',         # Adjust if necessary\n",
    "        filter_glob=\"*.bvh\",\n",
    "        target='ARMATURE',\n",
    "        global_scale=scale,\n",
    "        frame_start=1,\n",
    "        use_fps_scale=True\n",
    "    )\n",
    "\n",
    "def set_armature_display():\n",
    "    for obj in bpy.context.scene.objects:\n",
    "        if obj.type == 'ARMATURE':\n",
    "            bpy.context.view_layer.objects.active = obj  # Make it the active object\n",
    "            obj.select_set(True)\n",
    "            # Ensure it's visible in the viewport\n",
    "            obj.hide_viewport = False\n",
    "            obj.hide_set(False)\n",
    "            # Change display mode for better visibility\n",
    "            obj.data.display_type = 'STICK'  # Other options: 'ENVELOPE', 'OCTAHEDRAL', etc.\n",
    "\n",
    "# Clear existing scene\n",
    "bpy.ops.wm.read_factory_settings(use_empty=True)\n",
    "\n",
    "# Import BVH file\n",
    "import_bvh(bvh_file, scale=10.0)\n",
    "\n",
    "# Adjust armature visibility and display settings\n",
    "set_armature_display()\n",
    "\n",
    "# Optionally save the Blender file\n",
    "output_blend = os.path.splitext(bvh_file)[0] + \".blend\"\n",
    "bpy.ops.wm.save_as_mainfile(filepath=output_blend)\n",
    "\n",
    "print(\"BVH file imported and processed successfully.\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write the Blender script to a temporary file\n",
    "    script_file = \"import_bvh_to_blender.py\"\n",
    "    with open(script_file, \"w\") as file:\n",
    "        file.write(blender_script)\n",
    "    \n",
    "    # Run Blender with the temporary script\n",
    "    try:\n",
    "        subprocess.run([blender_executable, \"--python\", script_file], check=True)\n",
    "    finally:\n",
    "        # Clean up the temporary script\n",
    "        if os.path.exists(script_file):\n",
    "            os.remove(script_file)\n",
    "\n",
    "# Example usage\n",
    "bvh_file = \"miscs/cxk_cache/cxk.bvh\"  # Replace with the correct path to your BVH file\n",
    "try:\n",
    "    open_blender_with_bvh(bvh_file)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to process BVH file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'miscs/h36m_cxk.bvh'\n",
    "h36m_skel = h36m_skeleton.H36mSkeleton()\n",
    "_ = h36m_skel.poses2bvh(pose3d_world, output_file=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
